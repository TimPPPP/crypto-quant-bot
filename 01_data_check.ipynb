{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c2963f-8ba5-47d8-8034-dd92badd0a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "# research/01_verify_coint.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fix path to import from src\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.utils.loader import DataLoader\n",
    "from src.utils.universe import get_liquid_universe\n",
    "\n",
    "# --- 1. CONFIG ---\n",
    "# Pick a pair the Scanner found (or test two known correlated assets)\n",
    "ASSET_Y = 'BTC' \n",
    "ASSET_X = 'ETH'\n",
    "LOOKBACK_DAYS = 60\n",
    "\n",
    "# --- 2. GET DATA ---\n",
    "print(\"Fetching Data...\")\n",
    "universe = [ASSET_Y, ASSET_X]\n",
    "loader = DataLoader(universe)\n",
    "df = loader.fetch_data(LOOKBACK_DAYS)\n",
    "\n",
    "# Extract Close prices and Log-Transform\n",
    "closes = df.xs('close', axis=1, level=1).ffill()\n",
    "log_prices = np.log(closes)\n",
    "\n",
    "# Split Train/Test (Like the scanner)\n",
    "train_len = int(len(log_prices) * 0.85)\n",
    "train = log_prices.iloc[:train_len]\n",
    "test = log_prices.iloc[train_len:]\n",
    "\n",
    "# --- 3. CALCULATE HEDGE RATIO (OLS) ---\n",
    "# Y = beta * X + alpha\n",
    "x_train = sm.add_constant(train[ASSET_X])\n",
    "model = sm.OLS(train[ASSET_Y], x_train).fit()\n",
    "hedge_ratio = model.params[ASSET_X]\n",
    "intercept = model.params['const']\n",
    "\n",
    "print(f\"Hedge Ratio: {hedge_ratio:.4f}\")\n",
    "\n",
    "# --- 4. CONSTRUCT THE SPREAD ---\n",
    "# Spread = Y - (beta * X) - alpha\n",
    "# We construct this for the WHOLE dataset to see how it performs Out-of-Sample\n",
    "spread = log_prices[ASSET_Y] - (hedge_ratio * log_prices[ASSET_X]) - intercept\n",
    "\n",
    "# --- 5. VISUALIZATION ---\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Normalized Prices (Visual Correlation)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(f\"Log-Prices: {ASSET_Y} vs {ASSET_X}\")\n",
    "plt.plot(train.index, train[ASSET_Y], label=ASSET_Y)\n",
    "plt.plot(train.index, train[ASSET_X], label=ASSET_X)\n",
    "plt.axvline(train.index[-1], color='r', linestyle='--', label='Train/Test Split')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: The Spread (The most important chart)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(f\"The Spread (Mean Reversion Check)\")\n",
    "plt.plot(spread.index, spread, color='purple', label='Spread')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(spread.std()*2, color='green', linestyle=':', label='+2 Std Dev')\n",
    "plt.axhline(spread.std()*-2, color='green', linestyle=':', label='-2 Std Dev')\n",
    "plt.axvline(train.index[-1], color='r', linestyle='--', label='Out-of-Sample Start')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6. STATS CHECK ---\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf = adfuller(spread.iloc[:train_len])\n",
    "print(f\"ADF P-Value (In-Sample): {adf[1]:.5f}\")\n",
    "print(f\"Spread Volatility: {spread.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba722a-87ea-41b5-b61d-e4172e15f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from src.features.engineering import FeatureEngineer\n",
    "from src.models.autoencoder import train_autoencoder\n",
    "from src.utils.universe import get_liquid_universe\n",
    "\n",
    "# --- INTERNAL HELPER: Doing the heavy lifting ---\n",
    "def _generate_clustering_results(lookback_days=50):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"--- 1. Data Extraction (Lookback: {lookback_days}d) ---\")\n",
    "    universe = get_liquid_universe(50)\n",
    "    engine = FeatureEngineer(universe)\n",
    "    engine.load_data(lookback_days=lookback_days) \n",
    "    \n",
    "    print(\"\\n--- 2. Strategic Feature Engineering ---\")\n",
    "    raw_features = engine.calculate_features()\n",
    "    \n",
    "    if raw_features.empty:\n",
    "        print(\"âŒ No features.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"\\n--- 3. Training AI Market Mapper ---\")\n",
    "    model, latent_space = train_autoencoder(raw_features)\n",
    "    print(\"   Latent Space Dimension: 4 (Rich Feature Extraction)\")\n",
    "    \n",
    "    # Scale Latent Space\n",
    "    latent_scaler = RobustScaler()\n",
    "    latent_scaled = latent_scaler.fit_transform(latent_space)\n",
    "    \n",
    "    print(\"\\n--- 4. Clustering Regimes ---\")\n",
    "    n_clusters = 7\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(latent_scaled)\n",
    "    \n",
    "    results = raw_features.copy()\n",
    "    results['cluster'] = clusters\n",
    "    \n",
    "    # Sort clusters by Beta for consistent ordering\n",
    "    cluster_perf = results.groupby('cluster')['beta'].mean()\n",
    "    sorted_map = {old: new for new, old in enumerate(cluster_perf.sort_values().index)}\n",
    "    results['cluster'] = results['cluster'].map(sorted_map)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- PUBLIC EXPORT: For the Cointegration Scanner ---\n",
    "def get_cluster_map(lookback_days=60):\n",
    "    results = _generate_clustering_results(lookback_days)\n",
    "    if results.empty: return {}\n",
    "        \n",
    "    cluster_map = {}\n",
    "    for c in sorted(results['cluster'].unique()):\n",
    "        coins = results[results['cluster'] == c].index.tolist()\n",
    "        cluster_map[c] = coins\n",
    "        \n",
    "    return cluster_map\n",
    "\n",
    "def run_clustering_pipeline():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- 1. Data Extraction ---\")\n",
    "    # ... ~30 lines of calculation code ...\n",
    "    results['cluster'] = results['cluster'].map(sorted_map)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    universe = get_liquid_universe(50)\n",
    "    engine = FeatureEngineer(universe)\n",
    "    engine.load_data(lookback_days=50) \n",
    "    \n",
    "    print(\"\\n--- 2. Strategic Feature Engineering ---\")\n",
    "    raw_features = engine.calculate_features()\n",
    "    \n",
    "    if raw_features.empty:\n",
    "        print(\"âŒ No features.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- 3. Training AI Market Mapper ---\")\n",
    "    # Training the brain to recognize patterns\n",
    "    model, latent_space = train_autoencoder(raw_features)\n",
    "    print(\"   Latent Space Dimension: 4 (Rich Feature Extraction)\")\n",
    "    \n",
    "    # --- FIX 1: Scale Latent Space ---\n",
    "    # Even if inputs are scaled, the latent space can have outliers (like STABLE).\n",
    "    # Scaling here ensures K-Means calculates distances fairly for the main cluster.\n",
    "    latent_scaler = RobustScaler()\n",
    "    latent_scaled = latent_scaler.fit_transform(latent_space)\n",
    "    \n",
    "    print(\"\\n--- 4. Clustering Regimes ---\")\n",
    "    # --- FIX 2: Increase Clusters to 7 ---\n",
    "    # 4 buckets was too few. We need buckets for:\n",
    "    # 1. Stable, 2. Leaders, 3. Memes, 4. Laggards, 5. Defensive Alts, 6. Momentum, 7. Outliers\n",
    "    n_clusters = 7\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(latent_scaled)\n",
    "    \n",
    "    results = raw_features.copy()\n",
    "    results['cluster'] = clusters\n",
    "    \n",
    "    # Sort clusters by Beta (Low Risk -> High Risk) for consistent ordering\n",
    "    cluster_perf = results.groupby('cluster')['beta'].mean()\n",
    "    sorted_map = {old: new for new, old in enumerate(cluster_perf.sort_values().index)}\n",
    "    results['cluster'] = results['cluster'].map(sorted_map)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"      AI GENERATED MARKET MAP (Sorted by Beta Risk)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for c in sorted(results['cluster'].unique()):\n",
    "        cluster_df = results[results['cluster'] == c]\n",
    "        coins = cluster_df.index.tolist()\n",
    "        avg = cluster_df.mean()\n",
    "        \n",
    "       # --- STRICT HIERARCHICAL LABELING (Refined V3) ---\n",
    "        \n",
    "        label = \"UNKNOWN\"\n",
    "        desc = \"General Market Movement\"\n",
    "        \n",
    "        # 1. SPECIAL CASES: Defensive\n",
    "        if avg['beta'] < 0.5:\n",
    "            label = \"ðŸ›¡ï¸ DEFENSIVE / HEDGE\"\n",
    "            desc = \"Uncorrelated or Safe Haven (Stablecoins/Gold)\"\n",
    "            \n",
    "        # 2. THE TRAP: Toxic Volatility (High Risk, Bad Returns)\n",
    "        # CRITICAL FIX: Check this BEFORE 'High Octane' to catch falling knives like Cluster 3\n",
    "        elif (avg['volatility_z'] > 1.2 or avg['beta'] > 1.3) and avg['alpha'] < 0:\n",
    "            label = \"âš ï¸ TOXIC VOLATILITY / DOWNTREND\"\n",
    "            desc = \"High Risk but Negative Alpha (Avoid/Short)\"\n",
    "\n",
    "        # 3. THE WINNERS: High Octane (High Risk, Good Returns)\n",
    "        # Must have positive Alpha to earn the \"High Octane\" badge\n",
    "        elif (avg['volatility_z'] > 1.2 or avg['beta'] > 1.3) and avg['alpha'] > 0:\n",
    "            label = \"ðŸ”¥ HIGH OCTANE / MEMES\"\n",
    "            desc = \"Aggressive Growth Leaders (High Beta Winners)\"\n",
    "            \n",
    "        # 4. TRUE LEADERS: Significant Outperformance\n",
    "        # Raised Alpha threshold from 0.10 -> 0.15 to exclude \"The Blob\" (Cluster 1)\n",
    "        elif avg['alpha'] > 0.15:\n",
    "            label = \"ðŸš€ TRUE MARKET LEADERS\"\n",
    "            desc = \"Strongest Trenders beating the Index significantly\"\n",
    "            \n",
    "        # 5. THE LAGGARDS: Bleeding Value\n",
    "        elif avg['alpha'] < -0.05:\n",
    "            label = \"ðŸ’¤ LAGGARDS / WEAK\"\n",
    "            desc = \"Underperforming the benchmark consistently\"\n",
    "            \n",
    "        # 6. MOMENTUM: Volume/RSI Anomalies\n",
    "        elif abs(avg['rsi_rel']) > 15 or abs(avg['volume_flow']) > 1.0:\n",
    "            label = \"âš¡ MOMENTUM PLAY\"\n",
    "            desc = \"High relative volume or RSI divergence\"\n",
    "            \n",
    "        # 7. THE BLOB: Broad Market\n",
    "        # If it didn't pass the high bar for 'Leader' (>0.15), it falls here.\n",
    "        # This captures BTC/ETH and the 35 coins moving with the tide.\n",
    "        else:\n",
    "            label = \"ðŸŒŠ BROAD MARKET / SYSTEMATIC\"\n",
    "            desc = \"Moving in lockstep with the Index (Beta ~1.0)\"\n",
    "\n",
    "        print(f\"\\nCLUSTER {c} : {label} ({len(coins)} coins)\")\n",
    "        print(f\"   {desc}\")\n",
    "        print(f\"Coins: {coins}\")\n",
    "        \n",
    "        # Stats Table\n",
    "        print(f\"   Beta: {avg['beta']:.2f} | Alpha: {avg['alpha']:.4f} | Vol Z: {avg['volatility_z']:.2f}\")\n",
    "        print(f\"   Flow: {avg['volume_flow']:.2f} | RSI Rel: {avg['rsi_rel']:.1f} | Funding: {avg['funding_sentiment']:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_clustering_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
